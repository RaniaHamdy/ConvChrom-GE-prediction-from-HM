{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNN Model Variations.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPwM11uzN673+gyw47c31iL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RaniaHamdy/Histone-modification/blob/master/CNN_Model_Variations.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rWqr6tRPhDrZ"
      },
      "source": [
        " gpu_info = !nvidia-smi\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "if gpu_info.find('failed') >= 0:\n",
        "  print('Select the Runtime > \"Change runtime type\" menu to enable a GPU accelerator, ')\n",
        "  print('and then re-execute this cell.')\n",
        "else:\n",
        "  print(gpu_info)\n",
        "from psutil import virtual_memory\n",
        "ram_gb = virtual_memory().total / 1e9\n",
        "print('Your runtime has {:.1f} gigabytes of available RAM\\n'.format(ram_gb))\n",
        "\n",
        "if ram_gb < 20:\n",
        "  print('To enable a high-RAM runtime, select the Runtime > \"Change runtime type\"')\n",
        "  print('menu, and then select High-RAM in the Runtime shape dropdown. Then, ')\n",
        "  print('re-execute this cell.')\n",
        "else:\n",
        "  print('You are using a high-RAM runtime!')\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')## data set in goole drive "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ukV90IZehgBN"
      },
      "source": [
        "pip install keras-self-attention"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-l6ZzdnhgHD"
      },
      "source": [
        "# Basic packages  \n",
        "import keras\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "import pandas as pd \n",
        "import numpy as np\n",
        "import os\n",
        "from keras_preprocessing import image\n",
        "from keras import preprocessing,datasets\n",
        "\n",
        "# Packages for data preparation\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers.core import Reshape\n",
        "from sklearn import preprocessing\n",
        "from keras_preprocessing import sequence\n",
        "# Package For Imbalanced Data Distribution\n",
        "from imblearn.over_sampling import SMOTE \n",
        "\n",
        "#from keras import KerasClassifier\n",
        "import keras.layers\n",
        "from keras.engine.topology import Layer\n",
        "from keras import layers\n",
        "from tensorflow.keras.models import Sequential, Model, load_model\n",
        "from tensorflow.keras.layers import Activation \n",
        "from tensorflow.keras.layers import Conv1D,Convolution1D, SeparableConv1D\n",
        "from tensorflow.keras.layers import MaxPooling1D,AveragePooling1D,GlobalAveragePooling1D,GlobalMaxPooling1D\n",
        "from tensorflow.keras.layers import SpatialDropout1D ,Dropout\n",
        "from tensorflow.keras.layers import Conv2D,Convolution2D,ConvLSTM2D,Conv2DTranspose, DepthwiseConv2D\n",
        "from tensorflow.keras.layers import AveragePooling2D,MaxPooling2D, GlobalMaxPooling2D,ZeroPadding2D\n",
        "from tensorflow.keras.layers import Bidirectional,Activation,Flatten,Dense ,BatchNormalization\n",
        "#from tensorflow.keras.layers import TimeDistribute\n",
        "from keras.layers import LSTM ,RepeatVector\n",
        "from tensorflow.compat.v1.keras.layers import CuDNNLSTM   # CuDNNLSTM not yet released for TF 2.0\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from tensorflow.python.keras import Input\n",
        "from keras.utils import layer_utils\n",
        "tf.compat.v1.keras.layers.Attention\n",
        "\n",
        "# Packages for Data overfitting\n",
        "from keras import regularizers\n",
        "# Packages for Data evaluation and tunning\n",
        "from sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score,cohen_kappa_score,roc_auc_score,confusion_matrix,roc_curve, auc\n",
        "from sklearn.model_selection import cross_val_score ,cross_val_predict ,cross_val_predict,GridSearchCV\n",
        "#Model tuning \n",
        "from tensorflow.keras.initializers import glorot_uniform ,he_uniform,glorot_normal ,RandomNormal\n",
        "from keras import optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard\n",
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "\n",
        "# Packages for Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "#import scipy.misc\n",
        "from matplotlib.pyplot import imshow\n",
        "from tensorflow.keras.utils import plot_model\n",
        "\n",
        "# problem in LStm graph(just for colap)\n",
        "print(\"Tensorflow version \" + tf.__version__)\n",
        "K.clear_session()\n",
        "tf.compat.v1.reset_default_graph()\n",
        "tf.compat.v1.enable_eager_execution()\n",
        "#print(tf.executing_eagerly())\n",
        "\n",
        "from keras import initializers\n",
        "from keras_self_attention import SeqSelfAttention\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "checkpointer = ModelCheckpoint(filepath=\"model_seqHMDilated.h5\",\n",
        "                               verbose=0,\n",
        "                              monitor='val_loss', save_weights_only=True, save_best_only=True)\n",
        "\n",
        "#my_callbacks = [\n",
        "    #tf.keras.callbacks.EarlyStopping(patience=2),\n",
        "    #tf.keras.callbacks.ModelCheckpoint(filepath=\"/content/gdrive/My Drive/model_seqHMDilated.h5\",monitor='val_loss',verbose=0, save_weights_only=True, save_best_only=True),\n",
        "    #tf.keras.callbacks.TensorBoard(log_dir='./logs'),]\n",
        "\n",
        "seed =123\n",
        "batch_size=200\n",
        "epochs = 10\n",
        "loss_list=[]\n",
        "accuracy_list=[]\n",
        "AUC_ev_list=[]\n",
        "seed =123\n",
        "np.random.seed(seed)\n",
        "Cell_type=56"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWEhR6SEhpDz"
      },
      "source": [
        "def list_data():\n",
        "    all_X_train_files=list(filter(lambda x: '_rc_x_train.csv' in x, os.listdir('/content/gdrive/My Drive/HM Dataset')))\n",
        "    all_X_train_files.sort() \n",
        "    all_y_train_files=list(filter(lambda x: '_rc_y_train.csv' in x, os.listdir('/content/gdrive/My Drive/HM Dataset')))\n",
        "    all_y_train_files.sort() \n",
        "    all_X_test_files=list(filter(lambda x: '_rc_x_test.csv' in x, os.listdir('/content/gdrive/My Drive/HM Dataset')))\n",
        "    all_X_test_files.sort()\n",
        "    all_y_test_files=list(filter(lambda x: '_rc_y_test.csv' in x, os.listdir('/content/gdrive/My Drive/HM Dataset')))\n",
        "    all_y_test_files.sort() \n",
        "    return all_X_train_files,all_y_train_files,all_X_test_files,all_y_test_files\n",
        "\n",
        "\n",
        "def load_data(fileindex):\n",
        "    all_X_train_files,all_y_train_files,all_X_test_files,all_y_test_files= list_data()\n",
        "    X_train_file = all_X_train_files[fileindex]\n",
        "    y_train_file = all_y_train_files[fileindex]\n",
        "    X_test_file = all_X_test_files[fileindex]\n",
        "    y_test_file = all_y_test_files[fileindex]\n",
        "    file_name = X_train_file\n",
        "    x_train = np.loadtxt('/content/gdrive/My Drive/HM Dataset' + os.sep + X_train_file, delimiter = \",\")\n",
        "    x_test  = np.loadtxt('/content/gdrive/My Drive/HM Dataset'+ os.sep + X_test_file , delimiter = \",\") \n",
        "    y_train = np.loadtxt('/content/gdrive/My Drive/HM Dataset' + os.sep + y_train_file, delimiter = \",\")\n",
        "    y_test  = np.loadtxt('/content/gdrive/My Drive/HM Dataset' + os.sep + y_test_file , delimiter = \",\") \n",
        "    return x_train,x_test,y_train,y_test,file_name\n",
        "  # using  data set for 1DCNN \n",
        "def split_data(fileindex): \n",
        "    x_train,x_test,y_train,y_test,file_name=load_data(fileindex)\n",
        "    \n",
        "    y_train= y_train[:,1] \n",
        "    y_test =  y_test[:,1]\n",
        "    #split X_train 5 column(5 Histone)\n",
        "    X_train = x_train[:,1:6] \n",
        "    X_test  = x_test[:,1:6]   \n",
        "\n",
        "    #count gene numbers each gene 100 bin\n",
        "    num_genes_train = X_train.shape[0] / 100\n",
        "    num_genes_test  = X_test.shape[0] / 100\n",
        "    #\"np.split\"This function divides the array into subarrays along a specified axis\n",
        "    X_train = np.split(X_train, num_genes_train)\n",
        "    X_test  = np.split(X_test, num_genes_test)\n",
        "\n",
        "\n",
        "    X_train = np.array(X_train)\n",
        "    X_test  = np.array(X_test)\n",
        "    # convert data from list to array\n",
        "    y_train = np.array(y_train)\n",
        "    y_test  = np.array(y_test)\n",
        "   # using  data set for 2DCNN \n",
        "   #X_train = np.reshape(X_train,(X_train.shape[0],X_train.shape[1],X_train.shape[2]))\n",
        "   #X_test = np.reshape(X_test,(X_test.shape[0],X_test.shape[1],X_test.shape[2]))\n",
        "\n",
        "    file_name=file_name\n",
        "    print(file_name)  \n",
        "    return y_train,y_test,X_train,X_test,file_name\n",
        "\n",
        "def SMOTE(X_train,y_train):\n",
        "  nsamples, nx, ny = X_train.shape\n",
        "  X_train2d = X_train.reshape((nsamples,nx*ny))\n",
        "  #print(X_train2d)\n",
        "  from imblearn.over_sampling import SMOTE \n",
        "  sm = SMOTE(random_state = 2) \n",
        "  X_train_res, y_train_res = sm.fit_sample(X_train2d, y_train.ravel()) \n",
        "  nsamples1, nx1 = X_train_res.shape\n",
        "  X_train=X_train_res.reshape((nsamples1,nx,ny))\n",
        "  return X_train,y_train_res\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rf5IXX0FE2a_"
      },
      "source": [
        "\n",
        "def CNN1D():\n",
        "  sequence_input = Input(shape=(x_tr.shape[1],x_tr.shape[2]))\n",
        "  x= Conv1D(50,5, padding='same', activation= 'relu')(sequence_input)#'same'\n",
        "  x= MaxPooling1D(pool_size=(2))(x)\n",
        "  x= Conv1D(40,3, padding='same' ,activation= 'relu')(x)\n",
        "  x = Dropout(0.4 )(x)\n",
        "  x= Conv1D(30,2, padding='same' ,activation= 'relu')(x)\n",
        "  x = Dropout(0.4,)(x)\n",
        "  x= Conv1D(20,2, padding='same' ,activation= 'relu')(x)\n",
        "  x = Dropout(0.4)(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = Dense(64)(x)\n",
        "\n",
        "  preds = Dense(1,activation='sigmoid')(x)\n",
        "  model = Model(sequence_input, preds)\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate= 0.005)\n",
        "  #model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=['accuracy',keras.metrics.AUC()])\n",
        "  model.compile(loss= 'binary_crossentropy' ,optimizer=optimizer ,metrics=['accuracy',keras.metrics.AUC()])\n",
        "  history=model.fit(x_tr, y_tr, batch_size=batch_size,epochs=epochs, validation_data=(x_va, y_va), verbose=1,callbacks = [checkpointer])\n",
        "  #history=model.fit(x_tr, y_tr, batch_size=batch_size,epochs=epochs, validation_data=(x_va, y_va), verbose=1,callbacks = [my_callbacks])\n",
        "  #model.summary()\n",
        "  return history,model#, hist_loss_tr,hist_loss_va ,hist_acc_tr,hist_acc_va# model,#, val_acc\n",
        "  #return model\n",
        "\n",
        "\n",
        "def get_auc():\n",
        "    history,model= CNN1D()\n",
        "    predictions_NN_prob = model.predict(X_test)\n",
        "    auc = roc_auc_score(y_test, predictions_NN_prob)\n",
        "    print(auc)\n",
        "    print('AUC: %.3f' % auc)\n",
        "    list_of_numbers.append(auc)\n",
        "    print('AUC=',list_of_numbers)\n",
        "    return list_of_numbers ,history,model#,false_positive_rate_list,recall_list ,roc_auc_list\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6g614XuE-vv"
      },
      "source": [
        "\n",
        "def CNN1Datt():\n",
        "  #x= Sequential()\n",
        "  sequence_input = Input(shape=(x_tr.shape[1],x_tr.shape[2]))\n",
        "  x= Conv1D(50,5, padding='same', activation= 'relu')(sequence_input)#'same'\n",
        "  x= MaxPooling1D(pool_size=(2))(x)\n",
        "  x= Conv1D(40,3, padding='same' ,activation= 'relu')(x)\n",
        "  x = Dropout(0.5 )(x)\n",
        "  x= Conv1D(30,2, padding='same' ,activation= 'relu')(x)\n",
        "  x = Dropout(0.4)(x)\n",
        "  x= Conv1D(20,2, padding='same' ,activation= 'relu')(x)\n",
        "  x=  SeqSelfAttention(attention_width=2,\n",
        "                       attention_type=SeqSelfAttention.ATTENTION_TYPE_MUL, attention_activation=None)(x)\n",
        "  x = Dropout(0.4)(x)\n",
        "\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(128)(x)\n",
        "  x = Dense(64)(x)\n",
        "\n",
        "  preds = Dense(1,activation='sigmoid')(x)\n",
        "  model = Model(sequence_input, preds)\n",
        "  optimizer=tf.keras.optimizers.Adam(learning_rate= 0.005)\n",
        "  model.compile(loss='binary_crossentropy',optimizer= optimizer ,metrics=['accuracy',keras.metrics.AUC()])\n",
        "  history=model.fit(x_tr, y_tr, batch_size=batch_size,epochs= epochs, validation_data=(x_va, y_va), verbose=1,callbacks = [checkpointer])\n",
        "  #model.summary()\n",
        "  return history,model#, hist_loss_tr,hist_loss_va ,hist_acc_tr,hist_acc_va# model,#, val_acc\n",
        "\n",
        "\n",
        "def get_auc():\n",
        "    history,model= CNN1Datt()\n",
        "    predictions_NN_prob = model.predict(X_test)\n",
        "    auc = roc_auc_score(y_test, predictions_NN_prob)\n",
        "    print(auc)\n",
        "    print('AUC: %.3f' % auc)\n",
        "\n",
        "    list_of_numbers.append(auc)\n",
        "    print('AUC=',list_of_numbers)\n",
        "    return list_of_numbers ,history,model#,false_positive_rate_list,recall_list ,roc_auc_list\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dtoX0KWIj8LI"
      },
      "source": [
        "def evaluate(model):\n",
        "  loss,accuracy,AUC_ev = model.evaluate(X_test, y_test)\n",
        "  print('Test Error Rate: ', round((1-accuracy)*100, 2))\n",
        "  #loss_list.append(loss)\n",
        "  #accuracy_list.append(accuracy)\n",
        "  AUC_ev_list.append(AUC_ev)\n",
        "  #print('loss=',loss_list)\n",
        "  #print('accuracy=',accuracy_list)\n",
        "  print('AUC_ev=',AUC_ev_list)\n",
        "  return accuracy_list,loss_list,AUC_ev_list\n",
        "\n",
        "list_of_numbers=[]\n",
        "for i in range(0, Cell_type):\n",
        "    y_train,y_test,X_train,X_test,file_name= split_data(i)\n",
        "    #X_train, y_train= SMOTE(X_train,y_train)\n",
        "    #split train set into train-val\n",
        "    x_tr, x_va, y_tr, y_va = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
        "    list_of_numbers ,history,model = get_auc()\n",
        "    avg_list_of_numbers = sum(list_of_numbers)/len(list_of_numbers)\n",
        "    #accuracy_list,loss_list,AUC_ev_list= evaluate(model)\n",
        "    #avg_loss_list = sum(loss_list)/len(loss_list)\n",
        "    #avg_accuracy_list= sum(accuracy_list)/len(accuracy_list)\n",
        "    #avg_loss_list= sum(loss_list)/len(loss_list)\n",
        "    #avg_AUC_ev_list= sum(AUC_ev_list)/len(AUC_ev_list)\n",
        "    avg = sum(list_of_numbers)/len(list_of_numbers)\n",
        "    print(\"The average is \", round(avg,5))\n",
        "    print(\"The avg_list_of_numbers is \", round(avg_list_of_numbers,5))\n",
        "    i+1\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ORr29q73cMlQ"
      },
      "source": [
        "seed =123\n",
        "np.random.seed(seed)\n",
        "def CNN2D():\n",
        "  #x= Sequential()\n",
        "  sequence_input = Input(shape=(1,100,5))\n",
        "  x= Conv2D(50, (5,5), padding='same')(sequence_input)\n",
        "  x= MaxPooling2D(pool_size=(1,1))(x) \n",
        "  x= Conv2D(40, (3,3), padding='same', activation= 'relu')(x)\n",
        "  x = Dropout(0.4)(x)\n",
        "  x= Conv2D(30, (2,2), padding='same', activation= 'relu')(x)\n",
        "  x = Dropout(0.4)(x)\n",
        "  x= Conv2D(20, (2,2), padding='same', activation= 'relu')(x)\n",
        "  x = Dropout(0.4)(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dense(128, activation=\"relu\")(x)\n",
        "  x = Dense(64, activation=\"relu\")(x)\n",
        "\n",
        "  x = Dropout(0.2)(x)\n",
        "\n",
        "  preds = Dense(1,activation='sigmoid')(x)\n",
        "  model = Model(sequence_input, preds)\n",
        "  model.compile(loss='binary_crossentropy',optimizer=tf.keras.optimizers.Adam(learning_rate= 0.005),metrics=['accuracy',keras.metrics.AUC()])\n",
        "  #model.summary()\n",
        "  model.fit(x_tr, y_tr, batch_size=batch_size,epochs=epochs, validation_data=(x_va, y_va), verbose=0,callbacks = [checkpointer])\n",
        "  return model\n",
        "\n",
        "predictions_NN_problist=[]\n",
        "def get_auc():\n",
        "    model= CNN2D()\n",
        "    predictions_NN_prob = model.predict(X_test)\n",
        "    auc = roc_auc_score(y_test, predictions_NN_prob)\n",
        "    # evaluate model\n",
        "    print(auc)\n",
        "    print('AUC: %.3f' % auc)   \n",
        "    predictions_NN_problist.append(predictions_NN_prob)\n",
        "    list_of_numbers.append(auc)\n",
        "    print(list_of_numbers)\n",
        "    return list_of_numbers ,predictions_NN_problist,model\n",
        "\n",
        "list_of_numbers=[]\n",
        "for i in range(0, Cell_type):\n",
        "    y_train,y_test,X_train,X_test,file_name= split_data(i)\n",
        "    #X_train,y_train=SMOTE(X_train,y_train)\n",
        "    #split train set into train-val\n",
        "    x_tr, x_va, y_tr, y_va = train_test_split(X_train, y_train, test_size=0.1, random_state=0)\n",
        "    list_of_numbers ,predictions_NN_problist,model = get_auc()\n",
        "    avg = sum(list_of_numbers)/len(list_of_numbers)\n",
        "    print(\"The average is \", round(avg,5))\n",
        "    i+1\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}